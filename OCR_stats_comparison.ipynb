{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR engine comparison\n",
    "\n",
    "<img src=\"https://media.arxiv-vanity.com/render-output/6158804/images/fig_2_no_trademarks.png\" alt=\"text to images\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from glob import glob\n",
    "# from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import Levenshtein as Lv\n",
    "from statistics import mean\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "1. Take a look at the data\n",
    "2. Extract text from images:\n",
    "    - pytesseract\n",
    "    - easyocr\n",
    "    - keras_ocr\n",
    "3. Run on a few examples and compare the results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the data\n",
    "TextOCR requires models to perform text-recognition on arbitrary shaped scene-text present on natural images. TextOCR provides ~1M high quality word annotations on TextVQA images allowing application of end-to-end reasoning on downstream tasks such as visual question answering or image captioning.\n",
    "\n",
    "- 28,134 natural images from TextVQA\n",
    "- 903,069 annotated scene-text words\n",
    "- 32 words per image on average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot = pd.read_parquet('train_val_images/annot.parquet')\n",
    "imgs = pd.read_parquet('train_val_images/img.parquet')\n",
    "img_fns = glob('train_val_images/train_images/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Example Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.imshow(plt.imread(img_fns[100]))\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = img_fns[100].split('/')[-1].split('\\\\')[-1].split('.')[0]\n",
    "annot[annot[\"image_id\"] == image_id]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display for first 25 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 5, figsize=(20, 20))\n",
    "axs = axs.flatten()\n",
    "for i in range(25):\n",
    "    axs[i].imshow(plt.imread(img_fns[i]))\n",
    "    axs[i].axis('off')\n",
    "    image_id = img_fns[i].split('/')[-1].split('\\\\')[-1].split('.')[0]\n",
    "    n_annot = len(annot.query('image_id == @image_id'))\n",
    "    axs[i].set_title(f'{image_id} - {n_annot}')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison method:\n",
    "Because the bounding box can be slightly different depending on the OCR engine, it would not be possible to to let the engines detect text, and compare the result to the ground truth by comparing it with the text in the bounding boxes provided by the dataset. Since words can repeat in a single picture, looking if a string is included in the annotations could also lead to false positives. \n",
    "To avoid these problems, the bounding boxes described by the dataset for the text will be used by the OCR engines as Regions Of Interet (ROIs). The resulting detected string can then be compared one-to-one to the base truth for this area of the picture.\n",
    "Working on ROIs rather than the whole picture can improve the accuracy of the detection and recognition of characters by reducing noise, leading to skewed comparison results, but since the use cases of this thesis will also make use of ROIs, this method is here appropriate to choose the best engine for our uses.\n",
    "\n",
    "### Statistical tests:\n",
    "Test on 100 randomly chosen pictures from dataset\n",
    "1. Word Error Rate (WER)\n",
    "2. Character Error Rate (CER) -> Levenshtein distance\n",
    "<br>-> Remove punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 100\n",
    "sample_idxs = random.sample(range(0, len(img_fns)), sample_size)\n",
    "\n",
    "class Img:\n",
    "    def __init__(self, idx):\n",
    "        self.image = cv2.imread(img_fns[idx])\n",
    "        self.image_id = img_fns[idx].split('/')[-1].split('\\\\')[-1].split('.')[0]\n",
    "        self.annotations = annot.query('image_id == @self.image_id')\n",
    "\n",
    "sample = [Img(idx) for idx in sample_idxs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[0].annotations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engine 1: pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytesseract import pytesseract\n",
    "pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('engine_acc_results/pytesseract_accuracy_test.txt', 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(f\"Pytesseract inaccurate results:\\n\")\n",
    "\n",
    "total_accurate = 0\n",
    "total_annot = 0\n",
    "total_char = 0\n",
    "lv_distances = []\n",
    "\n",
    "for img in sample[:20]:\n",
    "\n",
    "    with open('engine_acc_results/pytesseract_accuracy_test.txt', 'a', encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Image {img.image_id}:\\n\")\n",
    "\n",
    "    img_accurate = 0\n",
    "    img_annot = len(img.annotations)\n",
    "    total_annot += img_annot\n",
    "\n",
    "    for _, row in img.annotations.iterrows():\n",
    "        \n",
    "        if row['utf8_string'] == \".\":\n",
    "            continue\n",
    "\n",
    "        # Pass ROI to engine\n",
    "        roi = [int(x) for x in row['bbox']]\n",
    "        roi_img = img.image[roi[1]:roi[1]+roi[3], roi[0]:roi[0]+roi[2]]\n",
    "        result = pytesseract.image_to_string(roi_img)\n",
    "\n",
    "        # Get Levenshtein distance\n",
    "        lv_distance = Lv.distance(row['utf8_string'].lower(), result.lower())\n",
    "        lv_distances.append(lv_distance)\n",
    "        \n",
    "        # Check if word recognition is correct\n",
    "        if result.lower() == row['utf8_string'].lower():\n",
    "            img_accurate += 1\n",
    "        else:\n",
    "            with open('engine_acc_results/pytesseract_accuracy_test.txt', 'a', encoding=\"utf-8\") as f:\n",
    "                f.write(f\"Expected: {row['utf8_string'].lower()}, actual: {result.lower()}, lv_distance: {lv_distance}\\n\")\n",
    "            \n",
    "        total_accurate += img_accurate\n",
    "        total_char += len(row['utf8_string'])\n",
    "\n",
    "    with open('engine_acc_results/pytesseract_accuracy_test.txt', 'a', encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    # print(f\"Image word accuracy = {img_accurate}/{img_annot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word accuracy = 0/581\n",
      "Sum of Levenshtein distances / Total number of characters = 2183 / 2537\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total word accuracy = {total_accurate}/{total_annot}\")\n",
    "print(f\"Sum of Levenshtein distances / Total number of characters = {sum(lv_distances)} / {total_char}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2: easyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA not available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "reader = easyocr.Reader(['en'], gpu = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image word accuracy = 0/0\n",
      "Image word accuracy = 3/13\n",
      "Image word accuracy = 5/114\n",
      "Image word accuracy = 0/0\n",
      "Image word accuracy = 2/42\n",
      "Image word accuracy = 7/73\n",
      "Image word accuracy = 1/12\n",
      "Image word accuracy = 0/0\n",
      "Image word accuracy = 0/0\n",
      "Image word accuracy = 0/35\n",
      "Image word accuracy = 3/61\n",
      "Image word accuracy = 25/29\n",
      "Image word accuracy = 1/15\n",
      "Image word accuracy = 1/40\n",
      "Image word accuracy = 2/76\n",
      "Image word accuracy = 1/8\n",
      "Image word accuracy = 0/0\n",
      "Image word accuracy = 0/2\n",
      "Image word accuracy = 3/21\n",
      "Image word accuracy = 3/40\n"
     ]
    }
   ],
   "source": [
    "with open('engine_acc_results/easy_accuracy_test.txt', 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(f\"EasyOCR inaccurate results:\\n\")\n",
    "\n",
    "total_accurate = 0\n",
    "total_annot = 0\n",
    "total_char = 0\n",
    "lv_distances = []\n",
    "\n",
    "for img in sample[:20]:\n",
    "\n",
    "    with open('engine_acc_results/easy_accuracy_test.txt', 'a', encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Image {img.image_id}:\\n\")\n",
    "\n",
    "    img_accurate = 0\n",
    "    img_annot = len(img.annotations)\n",
    "    total_annot += img_annot\n",
    "\n",
    "    for _, row in img.annotations.iterrows():\n",
    "        \n",
    "        # Pass ROI to engine\n",
    "        roi = [int(x) for x in row['bbox']]\n",
    "        roi_img = img.image[roi[1]:roi[1]+roi[3], roi[0]:roi[0]+roi[2]]\n",
    "        result = reader.readtext(roi_img)\n",
    "        result_df = pd.DataFrame(result, columns=['bbox','text','conf'])\n",
    "        result = ''.join(result_df['text'])\n",
    "        \n",
    "        # Get Levenshtein distance\n",
    "        lv_distance = Lv.distance(row['utf8_string'].lower(), result.lower())\n",
    "        lv_distances.append(lv_distance)\n",
    "        \n",
    "        # Check if word recognition is correct\n",
    "        if result.lower() == row['utf8_string'].lower():\n",
    "            img_accurate += 1\n",
    "        else:\n",
    "            with open('engine_acc_results/easy_accuracy_test.txt', 'a', encoding=\"utf-8\") as f:\n",
    "                f.write(f\"Expected: {row['utf8_string'].lower()}, actual: {result.lower()}, lv_distance: {lv_distance}\\n\")\n",
    "\n",
    "        total_accurate += img_accurate\n",
    "        total_char += len(row['utf8_string'])\n",
    "\n",
    "    with open('engine_acc_results/easy_accuracy_test.txt', 'a', encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    print(f\"Image word accuracy = {img_accurate}/{img_annot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total word accuracy = {total_accurate}/{total_annot}\")\n",
    "print(f\"Sum of Levenshtein distances / Total number of characters = {sum(lv_distances)} / {total_char}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 3: keras_ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_ocr\n",
    "\n",
    "pipeline = keras_ocr.pipeline.Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pipeline.recognize([img_fns[100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results[0], columns=['text', 'bbox'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
